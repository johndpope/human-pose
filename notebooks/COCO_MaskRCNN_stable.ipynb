{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0, 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN, MaskRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "# import torchvision.datasets as dset\n",
    "\n",
    "sys.path.append('../maskrcnntools')\n",
    "import transforms as T\n",
    "from coco import CocoDetection\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../lib/coco2voc')\n",
    "from coco2voc_aux import annsToSeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to COCO\n",
    "COCO_DATASET = '../../../datasets/COCO/'\n",
    "TRAIN_DIR = os.path.join(COCO_DATASET, 'train2017')\n",
    "VAL_DIR = os.path.join(COCO_DATASET, 'val2017')\n",
    "ANNOTATIONS_DIR = os.path.join(COCO_DATASET, 'annotations')\n",
    "TRAIN_ANNOT = os.path.join(ANNOTATIONS_DIR, 'instances_train2017.json')\n",
    "VAL_ANNOT = os.path.join(ANNOTATIONS_DIR, 'instances_val2017.json')\n",
    "\n",
    "# path to logging dir\n",
    "TB_LOGS_DIR = '../logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=13.99s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_train = CocoDetection(root=TRAIN_DIR,\n",
    "                           annFile=TRAIN_ANNOT)\n",
    "coco_val = CocoDetection(root=VAL_DIR,\n",
    "                         annFile=VAL_ANNOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(object):\n",
    "    def __init__(self, torch_dataset, transforms=None):\n",
    "        self.torch_dataset = torch_dataset\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.torch_dataset[idx]\n",
    "        \n",
    "        masks = annsToSeg(target, coco_train.coco)[3]\n",
    "        masks = masks.astype(bool)\n",
    "        obj_ids = [x['category_id'] for x in target]\n",
    "        \n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = [x['category_id'] for x in target]\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        \n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        iscrowd = [x['iscrowd'] for x in target]\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.torch_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.5137, 0.5255, 0.5255,  ..., 0.0000, 0.0039, 0.0078],\n",
       "          [0.5137, 0.5294, 0.5373,  ..., 0.0039, 0.0118, 0.0196],\n",
       "          [0.5176, 0.5333, 0.5412,  ..., 0.0000, 0.0000, 0.0039],\n",
       "          ...,\n",
       "          [0.0392, 0.0157, 0.0000,  ..., 0.0275, 0.0235, 0.0118],\n",
       "          [0.0118, 0.0235, 0.0510,  ..., 0.0275, 0.0196, 0.0196],\n",
       "          [0.0078, 0.0078, 0.0039,  ..., 0.0275, 0.0196, 0.0157]],\n",
       " \n",
       "         [[0.6588, 0.6706, 0.6706,  ..., 0.0863, 0.0902, 0.0941],\n",
       "          [0.6588, 0.6745, 0.6824,  ..., 0.0863, 0.0902, 0.0980],\n",
       "          [0.6627, 0.6784, 0.6863,  ..., 0.0745, 0.0784, 0.0824],\n",
       "          ...,\n",
       "          [0.0157, 0.0392, 0.0431,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0078, 0.0078, 0.0314,  ..., 0.0118, 0.0000, 0.0000],\n",
       "          [0.0196, 0.0118, 0.0118,  ..., 0.0118, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.7647, 0.7725, 0.7725,  ..., 0.4235, 0.4275, 0.4275],\n",
       "          [0.7608, 0.7765, 0.7843,  ..., 0.4353, 0.4392, 0.4392],\n",
       "          [0.7647, 0.7804, 0.7882,  ..., 0.4353, 0.4275, 0.4314],\n",
       "          ...,\n",
       "          [0.1255, 0.1412, 0.1451,  ..., 0.0196, 0.0196, 0.0157],\n",
       "          [0.0784, 0.0980, 0.1490,  ..., 0.0157, 0.0157, 0.0235],\n",
       "          [0.0784, 0.0902, 0.1020,  ..., 0.0078, 0.0157, 0.0196]]]),\n",
       " {'boxes': tensor([[ 28., 188., 639., 473.],\n",
       "          [ 10.,   4., 328., 232.],\n",
       "          [ 75., 229., 390., 473.],\n",
       "          [207.,  14., 640., 387.],\n",
       "          [189.,  40., 264.,  86.],\n",
       "          [117.,  39., 174.,  84.],\n",
       "          [171.,  74., 253., 143.],\n",
       "          [183.,   2., 276.,  72.]]),\n",
       "  'labels': tensor([51, 51, 56, 51, 55, 55, 55, 55]),\n",
       "  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n",
       "  'image_id': tensor([0]),\n",
       "  'area': tensor([174135.,  72504.,  76860., 161509.,   3450.,   2565.,   5658.,   6510.]),\n",
       "  'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0])})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = CocoDataset(coco_train, get_transform(train=True))\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# FasterRCNN needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "mask_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                     output_size=14,\n",
    "                                                     sampling_ratio=2)\n",
    "# put the pieces together inside a FasterRCNN model\n",
    "model = MaskRCNN(backbone,\n",
    "                 num_classes=172,\n",
    "                 rpn_anchor_generator=anchor_generator,\n",
    "                 box_roi_pool=roi_pooler,\n",
    "                 mask_roi_pool=mask_roi_pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = CocoDataset(coco_train, get_transform(train=True))\n",
    "dataset_val = CocoDataset(coco_val, get_transform(train=False))\n",
    "\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 172\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = get_instance_segmentation_model(num_classes)\n",
    "# model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.00001,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "logger = Logger(TB_LOGS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "import time\n",
    "\n",
    "def start_tensorboard(command):\n",
    "    time.sleep(10)\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitor here:\n",
      "\ttensorboard --logdir=\"../logs\"\n"
     ]
    }
   ],
   "source": [
    "print('Monitor here:')\n",
    "print('\\ttensorboard --logdir=\"{}\"'.format(TB_LOGS_DIR))\n",
    "\n",
    "command = 'tensorboard --logdir=\"{}\"'.format(TB_LOGS_DIR)\n",
    "p = Process(target=start_tensorboard, args=[command,])\n",
    "p.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [    0/58633]  eta: 2 days, 14:43:06  lr: 0.000001  loss: 6.7490 (6.7490)  loss_classifier: 5.0637 (5.0637)  loss_box_reg: 0.0628 (0.0628)  loss_mask: 0.8354 (0.8354)  loss_objectness: 0.6870 (0.6870)  loss_rpn_box_reg: 0.1002 (0.1002)  time: 3.8508  data: 0.3461  max mem: 4234\n",
      "Epoch: [0]  [   10/58633]  eta: 10:41:27  lr: 0.000004  loss: 6.9725 (7.0580)  loss_classifier: 5.0576 (5.0527)  loss_box_reg: 0.0628 (0.0820)  loss_mask: 0.9359 (0.9592)  loss_objectness: 0.7010 (0.7032)  loss_rpn_box_reg: 0.1602 (0.2608)  time: 0.6565  data: 0.0390  max mem: 5239\n",
      "Epoch: [0]  [   20/58633]  eta: 8:07:10  lr: 0.000006  loss: 6.9725 (6.9916)  loss_classifier: 5.0272 (5.0252)  loss_box_reg: 0.0617 (0.0866)  loss_mask: 0.9553 (0.9826)  loss_objectness: 0.6956 (0.6985)  loss_rpn_box_reg: 0.1325 (0.1988)  time: 0.3311  data: 0.0072  max mem: 5239\n",
      "Epoch: [0]  [   30/58633]  eta: 7:08:59  lr: 0.000009  loss: 6.8829 (6.9374)  loss_classifier: 4.9336 (4.9750)  loss_box_reg: 0.0626 (0.0809)  loss_mask: 0.9776 (0.9836)  loss_objectness: 0.6880 (0.6967)  loss_rpn_box_reg: 0.1156 (0.2011)  time: 0.3197  data: 0.0061  max mem: 5239\n",
      "Epoch: [0]  [   40/58633]  eta: 6:42:07  lr: 0.000011  loss: 6.6896 (6.8456)  loss_classifier: 4.7707 (4.9038)  loss_box_reg: 0.0689 (0.0807)  loss_mask: 0.9333 (0.9677)  loss_objectness: 0.6890 (0.6955)  loss_rpn_box_reg: 0.1108 (0.1979)  time: 0.3205  data: 0.0061  max mem: 5239\n",
      "Epoch: [0]  [   50/58633]  eta: 7:27:00  lr: 0.000014  loss: 6.3715 (6.7390)  loss_classifier: 4.5147 (4.8089)  loss_box_reg: 0.0689 (0.0803)  loss_mask: 0.9185 (0.9587)  loss_objectness: 0.6836 (0.6936)  loss_rpn_box_reg: 0.1008 (0.1976)  time: 0.4867  data: 0.0059  max mem: 5239\n",
      "Epoch: [0]  [   60/58633]  eta: 7:05:48  lr: 0.000016  loss: 6.1733 (6.6292)  loss_classifier: 4.2428 (4.6844)  loss_box_reg: 0.0669 (0.0820)  loss_mask: 0.9397 (0.9688)  loss_objectness: 0.6782 (0.6914)  loss_rpn_box_reg: 0.1218 (0.2026)  time: 0.4862  data: 0.0058  max mem: 5239\n",
      "Epoch: [0]  [   70/58633]  eta: 6:51:01  lr: 0.000019  loss: 5.7669 (6.4693)  loss_classifier: 3.7931 (4.5231)  loss_box_reg: 0.0803 (0.0858)  loss_mask: 0.9536 (0.9666)  loss_objectness: 0.6746 (0.6895)  loss_rpn_box_reg: 0.1516 (0.2043)  time: 0.3275  data: 0.0059  max mem: 5239\n",
      "Epoch: [0]  [   80/58633]  eta: 6:38:49  lr: 0.000021  loss: 5.0880 (6.2695)  loss_classifier: 3.1449 (4.3169)  loss_box_reg: 0.0991 (0.0893)  loss_mask: 0.9161 (0.9566)  loss_objectness: 0.6752 (0.6885)  loss_rpn_box_reg: 0.1632 (0.2183)  time: 0.3249  data: 0.0059  max mem: 5239\n",
      "Epoch: [0]  [   90/58633]  eta: 6:29:41  lr: 0.000024  loss: 4.3243 (6.0025)  loss_classifier: 2.3995 (4.0616)  loss_box_reg: 0.0875 (0.0882)  loss_mask: 0.8788 (0.9492)  loss_objectness: 0.6687 (0.6859)  loss_rpn_box_reg: 0.1727 (0.2176)  time: 0.3223  data: 0.0059  max mem: 5239\n",
      "Epoch: [0]  [  100/58633]  eta: 6:54:13  lr: 0.000026  loss: 3.2938 (5.7083)  loss_classifier: 1.5005 (3.7860)  loss_box_reg: 0.0729 (0.0894)  loss_mask: 0.8633 (0.9417)  loss_objectness: 0.6568 (0.6826)  loss_rpn_box_reg: 0.1157 (0.2085)  time: 0.4891  data: 0.0060  max mem: 5239\n",
      "Epoch: [0]  [  110/58633]  eta: 6:47:21  lr: 0.000029  loss: 2.8613 (5.4465)  loss_classifier: 1.0530 (3.5247)  loss_box_reg: 0.0802 (0.0914)  loss_mask: 0.8643 (0.9414)  loss_objectness: 0.6489 (0.6796)  loss_rpn_box_reg: 0.1128 (0.2094)  time: 0.5007  data: 0.0060  max mem: 5392\n",
      "Epoch: [0]  [  120/58633]  eta: 6:40:45  lr: 0.000031  loss: 2.6846 (5.2036)  loss_classifier: 0.7801 (3.2933)  loss_box_reg: 0.0838 (0.0921)  loss_mask: 0.8712 (0.9360)  loss_objectness: 0.6423 (0.6764)  loss_rpn_box_reg: 0.1398 (0.2058)  time: 0.3420  data: 0.0060  max mem: 5392\n",
      "Epoch: [0]  [  130/58633]  eta: 6:34:59  lr: 0.000034  loss: 2.3930 (4.9876)  loss_classifier: 0.7017 (3.0898)  loss_box_reg: 0.0896 (0.0934)  loss_mask: 0.8249 (0.9292)  loss_objectness: 0.6292 (0.6723)  loss_rpn_box_reg: 0.1283 (0.2029)  time: 0.3355  data: 0.0062  max mem: 5392\n",
      "Epoch: [0]  [  140/58633]  eta: 6:29:20  lr: 0.000036  loss: 2.3747 (4.8042)  loss_classifier: 0.5873 (2.9183)  loss_box_reg: 0.1052 (0.0949)  loss_mask: 0.8295 (0.9225)  loss_objectness: 0.6148 (0.6684)  loss_rpn_box_reg: 0.1043 (0.2002)  time: 0.3294  data: 0.0061  max mem: 5392\n",
      "Epoch: [0]  [  150/58633]  eta: 6:44:55  lr: 0.000039  loss: 2.4245 (4.6423)  loss_classifier: 0.6309 (2.7677)  loss_box_reg: 0.1094 (0.0963)  loss_mask: 0.8200 (0.9152)  loss_objectness: 0.6126 (0.6647)  loss_rpn_box_reg: 0.0987 (0.1984)  time: 0.4831  data: 0.0060  max mem: 5392\n",
      "Epoch: [0]  [  160/58633]  eta: 6:39:34  lr: 0.000041  loss: 2.3743 (4.4941)  loss_classifier: 0.5425 (2.6310)  loss_box_reg: 0.0964 (0.0969)  loss_mask: 0.8033 (0.9097)  loss_objectness: 0.6006 (0.6606)  loss_rpn_box_reg: 0.1262 (0.1959)  time: 0.4849  data: 0.0061  max mem: 5392\n",
      "Loss is nan, stopping training\n",
      "{'loss_classifier': tensor(0.5550, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>), 'loss_mask': tensor(0.7805, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_objectness': tensor(0.6675, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(2.5004, device='cuda:0', grad_fn=<DivBackward0>)}\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=10, lr_schedule='cyclic', logger=logger)\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_val, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
